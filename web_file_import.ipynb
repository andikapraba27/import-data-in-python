{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data From The Web"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we will try to import files from the web. In the first part, we will use wine data from datacamp that already save in amazon. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlretrieve\n",
    "import pandas as pd\n",
    "\n",
    "url = 'https://s3.amazonaws.com/assets.datacamp.com/production/course_1606/datasets/winequality-red.csv'\n",
    "urlretrieve(url, 'winequality-red.csv')\n",
    "df = pd.read_csv('winequality-red.csv', sep=';')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first step, we retrieve flat files from the web and save it to our local directory. What if we only need to load the data to python and didn't save it?. Well we can do it using pandas. We already save the url in the url variable in previous step. We can input this url to the read_csv function in pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "df = pd.read_csv(url, sep=';')\n",
    "print(df.head())\n",
    "\n",
    "pd.DataFrame.hist(df.ix[:,0:1])\n",
    "plt.xlabel('fixed acidity (g(tartaric acid)/dm$^3$)')\n",
    "plt.ylabel('count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could read excel file from the web. In the next part, we will done this using pandas and excel file from datacamp in amazon. Excel file will be read as a dictionary in python with sheet as the key and the tabels as the dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://s3.amazonaws.com/assets.datacamp.com/course/importing_data_into_r/latitude.xls'\n",
    "xl = pd.read_excel(url, sheet_name=None)\n",
    "print(xl.keys())\n",
    "print(xl['1700'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data From The Web HTML "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen, Request\n",
    "url = 'http://www.datacamp.com/teach/documentation'\n",
    "request = Request(url)\n",
    "response = urlopen(request)\n",
    "print(type(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = response.read()\n",
    "print(html)\n",
    "response.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "url = 'http://www.datacamp.com/teach/documentation'\n",
    "r = requests.get(url)\n",
    "text = r.text\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import HTML data using BeautifulSoup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "url = 'https://www.python.org/~guido/'\n",
    "r = requests.get(url)\n",
    "html_doc = r.text\n",
    "soup = BeautifulSoup(html_doc, 'lxml')\n",
    "pretty_soup = soup.prettify()\n",
    "print(pretty_soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guido_title = soup.title\n",
    "print(guido_title)\n",
    "\n",
    "guido_text = soup.get_text()\n",
    "print(guido_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_tags = soup.find_all('a')\n",
    "for link in a_tags:\n",
    "    print(link.get('href'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JSON File "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('data.json') as json_file:\n",
    "    json_data = json.load(json_file)\n",
    "\n",
    "print(json_data.keys())\n",
    "print(json_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API (Application programming Interface)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "url = 'http://www.omdbapi.com/?apikey=????????&t=the+social+network'\n",
    "r = requests.get(url)\n",
    "print(r.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data = r.json()\n",
    "for k in json_data.keys():\n",
    "    print(k + ':', json_data[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://en.wikipedia.org/w/api.php?action=query&prop=extracts&format=json&exintro=&titles=pizza'\n",
    "r = requests.get(url)\n",
    "json_data = r.json()\n",
    "print(json_data['query']['pages']['24768']['extract'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "# Store OAuth authentication credentials in relevant variables\n",
    "access_token = \"-\"\n",
    "access_token_secret = \"-\"\n",
    "consumer_key = \"-\"\n",
    "consumer_secret = \"-\"\n",
    "\n",
    "# Pass OAuth details to tweepy's OAuth handler\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token,access_token_secret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyStreamListener(tweepy.StreamListener):\n",
    "\n",
    "        def on_status(self, status):\n",
    "                print(status.text)\n",
    "# Initialize Stream listener\n",
    "l = MyStreamListener()\n",
    "\n",
    "# Create your Stream object with authentication\n",
    "stream = tweepy.Stream(auth, l)\n",
    "\n",
    "\n",
    "# Filter Twitter Streams to capture data by the keywords:\n",
    "stream.filter(track=['clinton','trump','sanders','cruz'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import package\n",
    "import json\n",
    "\n",
    "# String of path to file: tweets_data_path\n",
    "tweets_data_path = 'tweets3.txt'\n",
    "\n",
    "# Initialize empty list to store tweets: tweets_data\n",
    "tweets_data = []\n",
    "\n",
    "# Open connection to file\n",
    "tweets_file = open(tweets_data_path, \"r\")\n",
    "\n",
    "# Read in tweets and store in list: tweets_data\n",
    "for line in tweets_file:\n",
    "    tweet = json.loads(line)\n",
    "    tweets_data.append(tweet)\n",
    "\n",
    "# Close connection to file\n",
    "tweets_file.close()\n",
    "\n",
    "# Print the keys of the first tweet dict\n",
    "print(tweets_data[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import package\n",
    "import pandas as pd\n",
    "\n",
    "# Build DataFrame of tweet texts and languages\n",
    "df = pd.DataFrame(tweets_data, columns=['text','lang'])\n",
    "\n",
    "# Print head of DataFrame\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def word_in_text(word, text):\n",
    "    word = word.lower()\n",
    "    match = re.search(word, text)\n",
    "\n",
    "    if match:\n",
    "        return True\n",
    "    return False\n",
    "# Initialize list to store tweet counts\n",
    "[clinton, trump, sanders, cruz] = [0, 0, 0, 0]\n",
    "# Iterate through df, counting the number of tweets in which\n",
    "# each candidate is mentioned\n",
    "for index, row in df.iterrows():\n",
    "    clinton += word_in_text('clinton', row['text'].lower())\n",
    "    trump += word_in_text('trump', row['text'].lower())\n",
    "    sanders += word_in_text('sanders', row['text'].lower())\n",
    "    cruz += word_in_text('cruz', row['text'].lower())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set(color_codes=True)\n",
    "cd = ['clinton', 'trump', 'sanders', 'cruz']\n",
    "\n",
    "ax = sns.barplot(cd, [clinton, trump, sanders, cruz])\n",
    "ax.set(ylabel='count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
